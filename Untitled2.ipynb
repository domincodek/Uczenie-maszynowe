{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d50jiy1xBFzF",
        "outputId": "c3068507-7116-46dd-c0cf-9f00900ae632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CGnvc_z-BEKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ixdSgfNvBEKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb200c0-e492-4bc1-d8f1-700c1d9e4d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 4s 0us/step\n",
            "169017344/169001437 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Me9ye24GBEKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256aa3cd-a72e-4128-bb99-42355aaa496b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ]
        }
      ],
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bQSkKGBKBEKl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks.\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation)\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OldYjofgBEKp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oshk55WRBEKr"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dDFieEABBEKs"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yVpa1Ad_BEKu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super(MLPMixerLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjmLjpHyBEKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c7a57b-ac27-4720-d991-f87d1f007518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 58/352 [===>..........................] - ETA: 5:16 - loss: 4.4126 - acc: 0.0432 - top5-acc: 0.1548"
          ]
        }
      ],
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSxZG0ZxBEKx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FNetLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(FNetLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply fourier transformations.\n",
        "        x = tf.cast(\n",
        "            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedfowrad network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1yPUQw9BEKy"
      },
      "outputs": [],
      "source": [
        "fnet_blocks = keras.Sequential(\n",
        "    [FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.001\n",
        "fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n",
        "history = run_experiment(fnet_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1SvZ0fTBEKz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class gMLPLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(gMLPLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.channel_projection1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim * 2),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.channel_projection2 = layers.Dense(units=embedding_dim)\n",
        "\n",
        "        self.spatial_projection = layers.Dense(\n",
        "            units=num_patches, bias_initializer=\"Ones\"\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def spatial_gating_unit(self, x):\n",
        "        # Split x along the channel dimensions.\n",
        "        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].\n",
        "        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n",
        "        # Apply layer normalization.\n",
        "        v = self.normalize2(v)\n",
        "        # Apply spatial projection.\n",
        "        v_channels = tf.linalg.matrix_transpose(v)\n",
        "        v_projected = self.spatial_projection(v_channels)\n",
        "        v_projected = tf.linalg.matrix_transpose(v_projected)\n",
        "        # Apply element-wise multiplication.\n",
        "        return u * v_projected\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(inputs)\n",
        "        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n",
        "        x_projected = self.channel_projection1(x)\n",
        "        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_spatial = self.spatial_gating_unit(x_projected)\n",
        "        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n",
        "        x_projected = self.channel_projection2(x_spatial)\n",
        "        # Add skip connection.\n",
        "        return x + x_projected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNBd4CfpBEK0"
      },
      "outputs": [],
      "source": [
        "gmlp_blocks = keras.Sequential(\n",
        "    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.003\n",
        "gmlp_classifier = build_classifier(gmlp_blocks)\n",
        "history = run_experiment(gmlp_classifier)"
      ]
    }
  ]
}